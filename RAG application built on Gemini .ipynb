{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9b4b85-9d17-468c-b244-73a1c5191e3e",
   "metadata": {},
   "source": [
    "# RAG application built on Gemini "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f7aaa-aeb0-4632-82c1-f0f9801af6ae",
   "metadata": {},
   "source": [
    "<img src=\"app.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4f89a-5132-4a0e-b742-794c26809619",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f790c2c-d349-4577-9540-136ffaf8c62b",
   "metadata": {},
   "source": [
    "This project leverages Retrieval-Augmented Generation (RAG) to enhance the performance of large language models (LLMs) by incorporating external knowledge from external databases or knowledge sources. The project focuses on improving the quality of generated responses by combining both retrieval (finding relevant information) and generation (producing coherent, contextually-aware outputs) processes.\n",
    "\n",
    "Through the use of retrieval mechanisms and RAG workflows, the goal is to create an intelligent system that can access external information in real time, providing more accurate and contextual responses for a range of natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dabf07-72c1-48f6-b1b5-1800d1a2b1dd",
   "metadata": {},
   "source": [
    "## Key Objectives:\n",
    "1. Integration of Retrieval and Generation:\n",
    "The core objective is to seamlessly integrate retrieval techniques, such as ChromaDB, with a language model to provide high-quality, context-aware outputs. The retrieval step fetches relevant data from a knowledge base or documents, while the generation step uses this data to create responses.\n",
    "\n",
    "2. Knowledge Base Integration:\n",
    "A key part of the project is the use of a knowledge base or vector database (e.g., ChromaDB) to store and retrieve relevant embeddings or information. By utilizing semantic search, the system can find the most relevant information for any given query.\n",
    "\n",
    "3. Enhanced Language Model Performance:\n",
    "By augmenting traditional language models (like GPT) with external knowledge through the retrieval step, the model can produce more informed and accurate answers, especially for specific or domain-related queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a99e9-b403-4255-bcb7-176128ec3084",
   "metadata": {},
   "source": [
    "## Components of the Project:\n",
    "1. Retrieval Mechanism:\n",
    "The retrieval phase involves searching a database of embeddings (e.g., ChromaDB), which has preprocessed knowledge or text. This helps identify the most relevant context for the language model to generate informed responses.\n",
    "\n",
    "2. Generation Mechanism:\n",
    "The generation phase involves using an LLM, such as GPT, to create coherent and accurate responses based on the retrieved information. The model utilizes both its internal knowledge and the external retrieved context to produce high-quality outputs.\n",
    "\n",
    "3. Embedding and Vector Storage:\n",
    "Embeddings (vector representations of data) are stored in a database, such as ChromaDB, which allows for fast retrieval and comparison to generate highly relevant responses based on queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefb73c-35a2-4889-8c35-5e9cb88fa7b8",
   "metadata": {},
   "source": [
    "# ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952b149-9def-40bd-b450-8556ac9f1c5b",
   "metadata": {},
   "source": [
    "ChromaDB is a vector database designed for storing and retrieving embeddings efficiently. Embeddings are dense vector representations of data (e.g., text, images, or other data types) that are generated by machine learning models. These embeddings allow for fast similarity search and retrieval, which is crucial for tasks like information retrieval or retrieval-augmented generation (RAG), where you need to find relevant information from large datasets or knowledge bases.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "1. Efficient Storage: It stores embeddings and associated metadata (e.g., text, documents, etc.).\n",
    "2. Similarity Search: It supports high-performance similarity search for vector-based queries, allowing you to find the closest matches to a given query.\n",
    "3. Scalability: ChromaDB is optimized for scalability and can handle large-scale datasets efficiently.\n",
    "4. Integration: It can easily integrate with machine learning pipelines, such as those used for language models and RAG, where it retrieves relevant information to generate contextually appropriate responses.\n",
    "   \n",
    "* Use Case in RAG: ChromaDB is used to store and retrieve relevant information (i.e., text data or embeddings) that can be fed into an LLM for more accurate and context-aware generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fc22f-8a55-4ff0-b48f-cdc8453156a4",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "LangChain\n",
    "LangChain is an open-source framework designed to simplify the development of applications that leverage large language models (LLMs) and external tools. It provides an abstraction layer to easily chain together different components, such as models, retrievers, and output processors, making it easier to create complex workflows involving LLMs.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "1. Model Abstraction: LangChain provides interfaces to integrate various language models (e.g., GPT, BERT, etc.) and makes it easy to swap models or fine-tune them for specific tasks.\n",
    "2. Multi-Step Workflows: It allows for chaining multiple actions, such as using LLMs for generation, interacting with external APIs, running custom logic, and so on. This is particularly useful for building end-to-end applications that require multiple steps beyond simple model inference.\n",
    "3. Retrieval-Augmented Generation (RAG): LangChain simplifies the integration of retrieval mechanisms, like ChromaDB, into your LLM-based workflows. It can retrieve documents or context before running the model to generate a more accurate response.\n",
    "4. Tool Integration: LangChain supports integration with other tools such as databases, APIs, and data processing tools (e.g., Pandas, SQL) to enhance model outputs.\n",
    "\n",
    "Use Case in RAG: LangChain makes it easy to build retrieval-augmented generation workflows by chaining together LLMs, document retrieval tools (like ChromaDB), and custom logic to ensure the LLM has access to external data when generating responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc095da2-955b-434c-b40f-33816abb33e4",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d49c22a-1ad0-4395-b93b-aa95660aa026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Research Paper.pdf\")\n",
    "data = loader.load()  # entire PDF is loaded as a single Document\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a374eb7c-e262-42bb-8f3f-308ba7dcdbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbcd67-6dc3-4847-8370-4836307728e4",
   "metadata": {},
   "source": [
    "### To split long documents into smaller, more manageable chunks that can be processed by an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8896ecf-24f6-4be7-a8ed-7ff6c5b645d6",
   "metadata": {},
   "source": [
    "##### chunk_size=1000: Each chunk will have a maximum of 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29633e3b-ff24-4ace-a09b-c03b6e28c5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  50\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split data\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "print(\"Total number of documents: \",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "100b7d1a-1209-49d4-99ed-c51bc233a938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Research Paper.pdf', 'page': 2}, page_content='and European countries. COVID-19 is a broad community of viruses containing a nucleus of\\ngenetic material surrounded by protein spikes envelope that gives crown appearance [3]. Airborne\\nillness might spread from common cold to pneumonia, and symptoms tend to be mild in most\\nindividuals.\\nSevere Acute Respiratory Syndrome, first described by China in 2003, and Middle East\\nRespiratory Syndrome, first by Saudi Arabia in 2012, are coronavirus forms that cause serious\\nillness [4]. The COVID-19 has a near resemblance to the bat coronaviruses and is known as\\nbats are the primary source. However, the origins of COVID-19 remain under investigation.\\nRecent evidence indicates that the transmission has spread to humans from illicit wildlife sold\\nin the Hunan Seafood Wholesale Market. The first time COVID-19 was reported in China in\\n2019 and initially occurred in a group of pneumonia associated people in the city of Wuhan')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f518c7e-dc80-4feb-a839-5a2e26a746c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Research Paper.pdf', 'page': 15}, page_content='CMC, 2022, vol.70, no.3 5319\\n[26] H. Thakkar, V. Shah, H. Yagnik and M. Shah “Comparative anatomisation of data mining and fuzzy\\nlogic techniques used in diabetes prognosis,”ClinicalEHealth, vol. 4, pp. 12–23, 2020.\\n[27] H. N. K. Al-Behadili and K. R. Ku-Mahamud, “Fuzzy unordered rule using greedy hill climb-\\ning feature selection method: An application to diabetes classification,” Journal of Information and\\nCommunicationTechnology, vol. 20, pp. 391–422, 2021.\\n[28] N. Chandgude and S. Pawar, “Diagnosis of diabetes using fuzzy inference system,” in2016Int.Conf.\\nonComputingCommunicationControlandAutomation(ICCUBEA) , India, pp. 1–6, 2016.\\n[29] W. M. Shaban, A. H. Rabie, A. I. Saleh and M. Abo-Elsoud, “Detecting COVID-19 patients based on\\nfuzzy inference engine and deep neural network,”AppliedSoftComputing , vol. 99, pp. 106906, 2021.\\n[30] H. Garg and D. Rani, “New generalised Bonferroni mean aggregation operators of complex intu-')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[48]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2707d780-046d-47c5-a513-55a0568b8396",
   "metadata": {},
   "source": [
    "##### Chroma: A vector storage system to persist embeddings for efficient search and retrieval.\n",
    "##### GoogleGenerativeAIEmbeddings: A class to generate embeddings using Google's generative AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1073ab7f-2632-4367-8dec-c19449d6ce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EhtishamRaza\\anaconda3\\envs\\env_langchain1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05168594419956207,\n",
       " -0.030764883384108543,\n",
       " -0.03062233328819275,\n",
       " -0.02802734263241291,\n",
       " 0.01813093200325966]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "#Get an API key: \n",
    "# Head to https://ai.google.dev/gemini-api/docs/api-key to generate a Google AI API key. Paste in .env file\n",
    "\n",
    "# Embedding models: https://python.langchain.com/v0.1/docs/integrations/text_embedding/\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]\n",
    "#vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "688b6e6a-d8ab-41fb-a665-b72c9c9b4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c674c5c-1b57-42e9-a99d-9e882c75da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "retrieved_docs = retriever.invoke(\"What is new in FIS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04c5c6bb-fd0e-45ec-b315-e3f7656e0329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a1c8321-1efd-4a11-9744-0d1a7c6f4e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMC, 2022, vol.70, no.3 5309\n",
      "Figure 1:IoT enabled smart monitoring of diseases empowered with FIS\n",
      "This equation alters the membership functions of fuzzy sets of Fever, Cough, Respiratory\n",
      "Rate, Headache, Sore Throat, Flu, Blood Pressure, and Diarrhea. We have defined some common\n",
      "methods which are used within the context of disease diagnosis. We have two phases for the\n",
      "disease diagnosis first phase is the training phase, and the second phase is the validation phase.\n",
      "We formulated the training phase as taking different diseases as input and sending it to the IoT\n",
      "layer. The raw data has been sent to the processing layer, where the outliers are removed from the\n",
      "data. Missing values are filled with mean, mod and average values. After the processing layer, the\n",
      "preprocessed data has been sent to the application layer; this layer comprises the prediction and\n",
      "performance layers. First, the cleaned data has been passed through the prediction layer. After the\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f991a1f-6ce9-4463-9941-b35014df94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0.3, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee17439-7bc3-4931-9f57-4ec7e82ce902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266e86e0-746b-4943-9470-fd842633ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db9500d-4c51-4a10-9b21-f1ef9c8f985e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fuzzy Inference System (FIS) architecture consists of a sensory layer collecting input parameters (fever, cough, etc.), a preprocessing layer handling noise and null values, and an application layer.  The application layer further divides into prediction and performance layers.  The FIS within the prediction layer uses fuzzy logic to diagnose diseases based on the input parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Can you tell me about the architecture of this Fuzzy Inference System?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cff65d0-2436-47f8-8572-6979a3378701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Fuzzy Inference System (FIS) is designed to diagnose various diseases, including COVID-19, Typhoid, Malaria, and Pneumonia.  It also has been used in other studies for diagnosing sepsis, heart disease, diabetes, and cholera.  The system uses symptoms and other patient attributes as input and predicts the likelihood of different diseases.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"For which type of diseases this Fuzzy Inference System is developed?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b19008d8-29fe-458e-b8c9-98f458f511f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are two authors with Muhammad in their name.  Hafiz Muhammad Ehtisham Raza and Muhammad Idrees are listed as authors of the article \"Disease Diagnosis System Using IoT Empowered with Fuzzy Inference System\".  This article can be found in *Computers, Materials & Continua*.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Is there any author with surname of Muhammad?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ca9a4ad-6097-44f0-aeb8-e13466e9b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article, \"Disease Diagnosis System Using IoT Empowered with Fuzzy Inference System,\" is published in Computers, Materials & Continua (CMC).  The provided text specifies \"CMC, 2022, vol.70, no.3\" in multiple locations.  Additionally, the DOI is provided as \"10.32604/cmc.2022.020344\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"In which Journel this paper is published?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748b895-91f2-4fa4-a054-909d29ffafb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
